{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel(r'M:\\Data\\OnBoard\\Data and Reports\\Napa Vine\\2019\\Napa Vine-FINAL Data-Client_01212021.xlsx',\n",
    "                       sheet_name = 'Main Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sys_RespNum', 'CCGID', 'interview_start_time', 'interview_end_time', 'day_of_week', 'date', 'DTYPE', 'RUNID', 'Route', 'Dir', 'Strata', 'wcode', 'weight', 'tweight', 'Lang', 'From', 'To', 'FromSchool', 'ToSchool', 'FromUni', 'ToUni', 'Start_lat', 'Start_lon', 'End_lat', 'End_lon', 'ToBusMode1', 'ToBusMode2', 'ToBusMode3', 'ToBusMode4', 'GetFirstBus', 'totalbusBEFORE', 'Route_Before_1', 'Route_Before_2', 'Route_Before_3', 'Route_Before_4', 'System_Before_1', 'System_Before_2', 'System_Before_3', 'System_Before_4', 'FromBusMode1', 'FromBusMode2', 'FromBusMode3', 'FromBusMode4', 'FromLastBus', 'totalbusAFTER', 'Route_After_1', 'Route_After_2', 'Route_After_3', 'Route_After_4', 'System_After_1', 'System_After_2', 'System_After_3', 'System_After_4', 'fare', 'farecat', 'Alternatives', 'UseVineMoreFrequency', 'UseVineMoreDestination', 'UseVineMoreEarlier', 'UseVineMoreLater', 'cars', 'hh', 'hhwork', 'birthyear', 'Age', 'hisp', 'ETH1', 'ETH2', 'ETH3', 'ETH4', 'income', 'langhh', 'engspk', 'livebay', 'home_lat', 'home_lon', 'sch', 'School_Name', 'school_lat', 'school_lon', 'work', 'WorkLat', 'WorkLong', 'workafter', 'workbefore', 'schafter', 'schbefore', 'hometime_c1', 'hometime_c2', 'gender', 'com', 'Mode']\n",
      "(338, 92)\n",
      "The data has 338 records, 338 unique CCGID\n",
      "If the data has duplicates records: False\n"
     ]
    }
   ],
   "source": [
    "print(list(df_raw))\n",
    "df = df_raw.copy()\n",
    "\n",
    "df.dropna(how='all', inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# check duplicates\n",
    "print('The data has {} records, {} unique CCGID'.format(df.shape[0], len(df.CCGID.unique())))\n",
    "print('If the data has duplicates records: {}'.format(not(df[df.duplicated()].shape[0] == 0)))\n",
    "\n",
    "df.rename(columns= {'CCGID': 'ID'}, inplace=True)\n",
    "\n",
    "# use interview_start_time as time_string\n",
    "df['survey_time'] = df['interview_start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_mode stats: \n",
      "walk                                               277\n",
      "Dropped off by someone (NOT a service)              24\n",
      "Bike                                                16\n",
      "Drove or rode with others and parked                12\n",
      "Dropped off by Uber, Lyft or a similiar service      5\n",
      "Drove alone and parked                               2\n",
      "Taxi                                                 2\n",
      "Name: access_mode, dtype: int64\n",
      "egress_mode stats: \n",
      "walk                                                  278\n",
      "Picked up by someone (NOT a service)                   17\n",
      "Bike                                                   16\n",
      "Drive alone in vehicle parked nearby                   13\n",
      "Picked up using Uber, Lyft, or similar service          9\n",
      "Drive or ride with others in vehicle parked nearby      4\n",
      "Taxi                                                    1\n",
      "Name: egress_mode, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# build 'access_mode' and 'egress_mode' variables based on relevant fields\n",
    "\n",
    "ToBusMode_dict = {1: 'walk',\n",
    "                  2: 'public transit',\n",
    "                  3: 'Drove alone and parked', \n",
    "                  4: 'Drove or rode with others and parked',\n",
    "                  5: 'Dropped off by someone (NOT a service)', \n",
    "                  6: 'Dropped off by Uber, Lyft or a similiar service', \n",
    "                  7: 'Bike', \n",
    "                  8: 'Taxi', \n",
    "                  9: 'Other', \n",
    "                  10: 'Company Shuttle'}\n",
    "\n",
    "\n",
    "FromBusMode_dict = {1: 'walk',\n",
    "                    2: 'public transit',\n",
    "                    3: 'Drive alone in vehicle parked nearby', \n",
    "                    4: 'Drive or ride with others in vehicle parked nearby',\n",
    "                    5: 'Picked up by someone (NOT a service)', \n",
    "                    6: 'Picked up using Uber, Lyft, or similar service', \n",
    "                    7: 'Bike', \n",
    "                    8: 'Taxi', \n",
    "                    9: 'Other', \n",
    "                    10: 'Company Shuttle'}\n",
    "\n",
    "FromLastBus_dict = {1: 'walk', \n",
    "                    2: 'Drive alone in vehicle parked nearby', \n",
    "                    3: 'Drive or ride with others in vehicle parked nearby', \n",
    "                    4: 'Picked up by someone (NOT a service)', \n",
    "                    5: 'Picked up using Uber, Lyft, or similar service', \n",
    "                    6: 'Bike', \n",
    "                    7: 'Taxi', \n",
    "                    8: 'Other',\n",
    "                    9: 'Company Shuttle'}\n",
    "\n",
    "df['ToBusMode1'].fillna(0, inplace=True)\n",
    "df['ToBusMode1'] = df['ToBusMode1'].apply(lambda x: int(x))\n",
    "df['ToBusMode1'] = df['ToBusMode1'].map(ToBusMode_dict)\n",
    "\n",
    "df['FromBusMode1'].fillna(0, inplace=True)\n",
    "df['FromBusMode1'] = df['FromBusMode1'].apply(lambda x: int(x))\n",
    "df['FromBusMode1'] = df['FromBusMode1'].map(FromBusMode_dict)\n",
    "\n",
    "df['FromLastBus'].fillna(0, inplace=True)\n",
    "df['FromLastBus'] = df['FromLastBus'].apply(lambda x: int(x))\n",
    "df['FromLastBus'] = df['FromLastBus'].map(FromLastBus_dict)\n",
    "\n",
    "\n",
    "df['access_mode'] = df['ToBusMode1']\n",
    "print('access_mode stats: \\n{}'.format(df.access_mode.value_counts()))\n",
    "\n",
    "df['egress_mode'] = df['FromBusMode1']\n",
    "df.loc[df.FromBusMode1 == 'public transit', 'egress_mode'] = df['FromLastBus']\n",
    "print('egress_mode stats: \\n{}'.format(df.egress_mode.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code transfer system and create columns for 'system + route' which will be used for canonical_route mapping\n",
    "\n",
    "system_dict = {1: 'BART',\n",
    "               2: 'Napa Vine',\n",
    "               3: 'County Connection',\n",
    "               4: 'FAST (Fairfield-Suisun Transit)',\n",
    "               5: 'SolTrans (Solano County Transit)',\n",
    "               6: 'Sonoma County Transit',\n",
    "               7: 'Tri Delta Transit',\n",
    "               8: 'Vacaville City Coach',\n",
    "               9: 'AC Transit',\n",
    "               10: 'Caltrain',\n",
    "               11: 'Golden Gate Transit',\n",
    "               12: 'Golden Gate Ferry',\n",
    "               13: 'SamTrans',\n",
    "               14: 'San Francisco Bay Ferry',\n",
    "               15: 'San Francisco Muni',\n",
    "               16: 'VTA',\n",
    "               17: 'WestCat',\n",
    "               18: 'WHEELS',\n",
    "               19: 'Other (type in next to route)',\n",
    "               20: 'Lake Transit',\n",
    "               21: 'Capitol Corridor'}\n",
    "\n",
    "for i in ['System_Before_1', 'System_Before_2', 'System_Before_3', 'System_Before_4',\n",
    "          'System_After_1', 'System_After_2', 'System_After_3', 'System_After_4']:\n",
    "    #print(i)\n",
    "    #print(df[i])\n",
    "    df[i] = df[i].fillna(0)\n",
    "    df[i].replace(to_replace = ' ', value = 0, inplace=True)\n",
    "    df[i] = df[i].apply(lambda x: int(x))\n",
    "    df[i+'_temp'] = df[i].map(system_dict)\n",
    "    #print(df[i+'_temp'])\n",
    "\n",
    "df['first_route_before_survey_board'] = df['System_Before_1_temp'] + '___' + df['Route_Before_1'].astype(str)\n",
    "df['second_route_before_survey_board'] = df['System_Before_2_temp'] + '___' + df['Route_Before_2'].astype(str)\n",
    "df['third_route_before_survey_board'] = df['System_Before_3_temp'] + '___' + df['Route_Before_3'].astype(str)\n",
    "df['first_route_after_survey_alight'] = df['System_After_1_temp'] + '___' + df['Route_After_1'].astype(str)\n",
    "df['second_route_after_survey_alight'] = df['System_After_2_temp'] + '___' + df['Route_After_2'].astype(str)\n",
    "df['third_route_after_survey_alight'] = df['System_After_3_temp'] + '___' + df['Route_After_3'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['29.0_NORTH', '2.0_LOOP', '3.0_LOOP', '29.0_SOUTH', '10.0_SOUTH',\n",
       "       '1.0_LOOP', '10.0_NORTH', '4.0_LOOP', '11.0_NORTH', '8.0_SOUTH',\n",
       "       '21.0_EAST', '5.0_LOOP', '8.0_NORTH', '21.0_WEST', '11.0_SOUTH',\n",
       "       '7.0_LOOP', '6.0_LOOP'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new field to represent complete survey route info (route + direction)\n",
    "df['route_dir'] = df.Route.astype(str) + '_' + df.Dir\n",
    "df.route_dir.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['American Indian/Alaska Native_NA_NA_NA', 'Hispanic_NA_NA_NA', 'White/Caucasian_NA_NA_NA', 'Black/African American_NA_NA_NA', 'White/Caucasian_Hispanic_NA_NA', 'Asian_NA_NA_NA', 'Mixed_NA_NA_NA', 'American Indian/Alaska Native_Hispanic_NA_NA', 'Hispanic_American Indian/Alaska Native_NA_NA', 'American Indian/Alaska Native_White/Caucasian_Hispanic_NA', 'American Indian/Alaska Native_Black/African American_White/Caucasian_Asian', 'Mixed_Hispanic_NA_NA', 'Black/African American_White/Caucasian_NA_NA', 'Hispanic_White/Caucasian_NA_NA', 'White/Caucasian_Hispanic_American Indian/Alaska Native_NA', 'Native Hawaiian/Pacific Islander_NA_NA_NA', 'Black/African American_White/Caucasian_Hispanic_NA', 'Native Hawaiian/Pacific Islander_Black/African American_Asian_NA', 'American Indian/Alaska Native_White/Caucasian_NA_NA', 'Refused_NA_NA_NA', 'Asian_Hispanic_NA_NA', 'Black/African American_Hispanic_NA_NA', 'Persian/Arab/North African/Middle Eastern_NA_NA_NA', 'Native Hawaiian/Pacific Islander_White/Caucasian_NA_NA', 'American Indian/Alaska Native_Black/African American_Asian_NA']\n"
     ]
    }
   ],
   "source": [
    "race_dict = {1: 'American Indian/Alaska Native',\n",
    "             2: 'Native Hawaiian/Pacific Islander',\n",
    "             3: 'Black/African American',\n",
    "             4: 'White/Caucasian',\n",
    "             5: 'Asian',\n",
    "             6: 'Other',\n",
    "             7: 'Hispanic',\n",
    "             8: 'Mixed',\n",
    "             9: 'Persian/Arab/North African/Middle Eastern',\n",
    "             0: 'Refused',\n",
    "             10: 'NA'}\n",
    "\n",
    "for i in ['ETH1', 'ETH2', 'ETH3', 'ETH4']:\n",
    "    df[i] = df[i].fillna(10)\n",
    "    df[i].replace(to_replace = ' ', value = 10, inplace=True)\n",
    "    df[i] = df[i].apply(lambda x: int(x))\n",
    "    df[i+'_temp'] = df[i].map(race_dict)\n",
    "    \n",
    "df['ETH_concat'] = df['ETH1_temp'] + '_' + df['ETH2_temp'] + '_' + df['ETH3_temp'] + '_' + df['ETH4_temp']\n",
    "print(list(df['ETH_concat'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>hisp</th>\n",
       "      <th>hisp_from_ETH</th>\n",
       "      <th>ETH1</th>\n",
       "      <th>ETH2</th>\n",
       "      <th>ETH3</th>\n",
       "      <th>ETH4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>101.0</td>\n",
       "      <td>1</td>\n",
       "      <td>hisp</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID hisp hisp_from_ETH  ETH1  ETH2  ETH3  ETH4\n",
       "78  101.0    1          hisp     4     7     1    10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>hisp</th>\n",
       "      <th>hisp_from_ETH</th>\n",
       "      <th>ETH1</th>\n",
       "      <th>ETH2</th>\n",
       "      <th>ETH3</th>\n",
       "      <th>ETH4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>9010.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>9011.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>9012.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>9020.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>9028.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>9029.0</td>\n",
       "      <td>2</td>\n",
       "      <td>non-hisp</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID hisp hisp_from_ETH  ETH1  ETH2  ETH3  ETH4\n",
       "0       9.0    2      non-hisp     1    10    10    10\n",
       "4      13.0    2      non-hisp     4    10    10    10\n",
       "193  9010.0    2      non-hisp     4    10    10    10\n",
       "194  9011.0    2      non-hisp     4    10    10    10\n",
       "195  9012.0    2      non-hisp     0    10    10    10\n",
       "197  9020.0    2      non-hisp     4    10    10    10\n",
       "203  9028.0    2      non-hisp     0    10    10    10\n",
       "204  9029.0    2      non-hisp     0    10    10    10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['hisp_from_ETH'] = 'non-hisp'\n",
    "df.loc[df.ETH_concat.str.contains('Hispanic',na=False), 'hisp_from_ETH'] = 'hisp'\n",
    "\n",
    "# check consistency between 'hisp' derived from ETH and the original 'hisp' variable and make corrections\n",
    "\n",
    "# records where ETH indicates 'hispanic' but 'hisp' is 1 (not hispanic)\n",
    "display(df.loc[(df.hisp_from_ETH == 'hisp') & (df.hisp != 2)][['ID','hisp','hisp_from_ETH',\n",
    "                                                                 'ETH1', 'ETH2', 'ETH3', 'ETH4']])\n",
    "# make hisp=2 for records where hisp_from_ETH == 'hisp'\n",
    "df.loc[df.hisp_from_ETH == 'hisp', 'hisp'] = 2\n",
    "\n",
    "# records where ETH doesn't indicate 'hispanic' but 'hisp' is 2 (is hispanic)\n",
    "# for these, keep the '2' value because the surveyed may leave 'ETH-hispanic' out when they feel\n",
    "# they have already provided the information in 'hisp'\n",
    "display(df.loc[(df.hisp_from_ETH != 'hisp') & (df.hisp == 2)][['ID','hisp','hisp_from_ETH',\n",
    "                                                                 'ETH1', 'ETH2', 'ETH3', 'ETH4']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create race_dmy_xx\n",
    "\n",
    "df['race_dmy_ind'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('American Indian/Alaska Native',na=False), 'race_dmy_ind'] = 1\n",
    "\n",
    "df['race_dmy_hwi'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('Native Hawaiian/Pacific Islander',na=False), 'race_dmy_hwi'] = 1\n",
    "\n",
    "df['race_dmy_blk'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('Black/African American',na=False), 'race_dmy_blk'] = 1\n",
    "\n",
    "df['race_dmy_wht'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('White/Caucasian',na=False), 'race_dmy_wht'] = 1\n",
    "\n",
    "df['race_dmy_asn'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('Asian',na=False), 'race_dmy_asn'] = 1\n",
    "\n",
    "df['race_dmy_mdl_estn'] = 0\n",
    "df.loc[df.ETH_concat.str.contains('Persian/Arab/North African/Middle Eastern',na=False), 'race_dmy_mdl_estn'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop interim columns and export\n",
    "df.drop(columns = ['System_Before_1_temp', 'System_Before_2_temp', 'System_Before_3_temp',\n",
    "                   'System_Before_4_temp', 'System_After_1_temp', 'System_After_2_temp',\n",
    "                   'System_After_3_temp', 'System_After_4_temp', 'ETH1_temp', 'ETH2_temp',\n",
    "                   'ETH3_temp', 'ETH4_temp', 'ETH_concat', 'hisp_from_ETH'], inplace=True)\n",
    "\n",
    "df.to_csv(r'M:\\Data\\OnBoard\\Data and Reports\\Napa Vine\\2019\\As CSV\\Napa Vine_FINAL Data_addCols_NO POUND OR SINGLE QUOTE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic_Variable that shouldn not exit:\n",
      "['interview_start_time' 'interview_end_time']\n",
      "\n",
      "sys_RespNum\n",
      "RUNID\n",
      "Route\n",
      "com\n",
      "access_mode\n",
      "egress_mode\n"
     ]
    }
   ],
   "source": [
    "# bring in standard dictionary to check field consistency\n",
    "\n",
    "# dictionary for Napa Vine survey\n",
    "var = pd.read_csv(r'M:\\Data\\OnBoard\\Data and Reports\\Napa Vine\\2019\\variable_dictionary.csv',\n",
    "                  encoding = \"ISO-8859-1\", engine='python')\n",
    "\n",
    "# standard dictionary\n",
    "var_standard = pd.read_csv(r'C:\\Users\\ywang\\Documents\\GitHub\\onboard-surveys\\make-uniform\\production\\Dictionary for Standard Database.csv')\n",
    "var_standard.columns = [x+'_s' for x in list(var_standard)]\n",
    "\n",
    "# merge\n",
    "var_merge = var.merge(var_standard, left_on='Generic_Variable', right_on='Generic_Variable_s', how='outer')\n",
    "\n",
    "# check if 'Generic_Variable' in Napa Vine dictionary matches the standard 'Generic_Variable'. chk1 should be empty\n",
    "chk1 = var_merge.loc[(var_merge.Generic_Variable.notnull()) & (var_merge.Generic_Variable_s.isnull())]\n",
    "print('Generic_Variable that shouldn not exit:')\n",
    "print(chk1.Generic_Variable.unique())\n",
    "print()\n",
    "\n",
    "# check if columns names in survey data matches 'Survey_Variable' in Napa Vine dictionary.\n",
    "# the following loops should not include variables that are needed for standardization\n",
    "\n",
    "for i in var.loc[var.Generic_Variable.notnull()]['Survey_Variable']:\n",
    "    if i not in list(df):\n",
    "        print(i)\n",
    "        \n",
    "for i in list(df):\n",
    "    if i not in list(var.Survey_Variable):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_clean = var[['operator', 'Survey_year', 'Survey_Variable', 'Survey_Response', \n",
    "                 'Generic_Variable', 'Generic_Response']].drop_duplicates()\n",
    "var_clean = var_clean.loc[var_clean.Generic_Variable.notnull()]\n",
    "\n",
    "# export\n",
    "# var_clean.to_csv(r'M:\\Data\\OnBoard\\Data and Reports\\Napa Vine\\2019\\vars_for_standard_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ywang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\ywang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\ywang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# create raw transfer routes file\n",
    "routes = pd.DataFrame(columns = ['survey_name'])\n",
    "for i in ['first_route_before_survey_board', 'second_route_before_survey_board',\n",
    "          'third_route_before_survey_board', 'first_route_after_survey_alight', \n",
    "          'second_route_after_survey_alight', 'third_route_after_survey_alight']:\n",
    "    route_unique = df[[i]]\n",
    "    route_unique.columns = ['survey_name']\n",
    "    routes = pd.concat([routes, route_unique])\n",
    "\n",
    "routes_clean = routes.loc[routes.survey_name.notnull()]\n",
    "routes_clean.drop_duplicates(inplace=True)\n",
    "routes_clean['survey'] = 'Napa Vine'\n",
    "routes_clean['survey_year'] = 2019\n",
    "\n",
    "print(routes_clean.shape)\n",
    "# routes_clean[['survey','survey_year','survey_name']].to_csv(r'C:\\Users\\ywang\\Documents\\R\\OnboardSurvey_2020Oct_yq\\Data and Reports\\Napa Vine\\2019\\all_routes_raw.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
